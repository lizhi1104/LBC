{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "texts_dir_path = './data_zhihu/depression_patients/'\n",
    "for i in range(1, 16):\n",
    "    with open(os.path.join(texts_dir_path, '{:02}.txt'.format(i)), 'r') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = {}\n",
    "vocabs_dir_path = './vocab'\n",
    "topns = [25, 50, 75, 100]\n",
    "thresholds = [0.7, 0.75, 0.8, 0.85]\n",
    "vocab_names = ['new_vocab_{}_{}'.format(topn, thresh) for topn in topns for thresh in thresholds]\n",
    "vocab_names.append('old_vocab')\n",
    "for vocab_name in vocab_names:\n",
    "    with open(os.path.join(vocabs_dir_path, vocab_name+'.dic'), 'r', encoding='utf-8') as f:\n",
    "        vocab = {}\n",
    "        for line in f.readlines()[83:]:\n",
    "            x = line.strip().split('\\t')\n",
    "            vocab[x[0]] = x[1:]\n",
    "        vocabs[vocab_name] = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_types = [\n",
    "1, # function (Function Words)\n",
    "\t2, # pronoun (Pronouns)\n",
    "\t\t3, # ppron (Personal Pronouns)\n",
    "\t\t\t4, # i (I)\n",
    "\t\t\t5, # we (We)\n",
    "\t\t\t6, # you (You)\n",
    "\t\t\t7, # shehe (SheHe)\n",
    "\t\t\t8, # they (They)\n",
    "\t\t\t19, # youpl\n",
    "\t\t9, # ipron (Impersonal Pronouns)\n",
    "\t11, # prep (Prepositions)\n",
    "\t12, # auxverb (Auxiliary Verbs)\n",
    "\t13, # adverb (Adverbs)\n",
    "\t14, # conj (Conjunctions)\n",
    "\t15, # negate (Negations)\n",
    "\t17, # quanunit\n",
    "\t18, # prepend\n",
    "\t131, # specart\n",
    "\t132, # tensem\n",
    "\t\t90, # focuspast (Past Focus)\n",
    "\t\t91, # focuspresent (Present Focus)\n",
    "\t\t92, # focusfuture (Future Focus)\n",
    "\t\t133, # progm\n",
    "\t140, # particle\n",
    "\t\t141, # modal_pa\n",
    "\t\t142, # general_pa\n",
    "# othergram (Other Grammar)\n",
    "\t22, # compare (Comparisons)\n",
    "\t23, # interrog (Interrogatives)\n",
    "\t24, # number (Numbers)\n",
    "\t25, # quant (Quantifiers)\n",
    "30, # affect (Affect)\n",
    "\t31, # posemo (Positive Emotions)\n",
    "\t32, # negemo (Negative Emotions)\n",
    "\t\t33, # anx (Anx)\n",
    "\t\t34, # anger (Anger)\n",
    "\t\t35, # sad (Sad)\n",
    "40, # social (Social)\n",
    "\t41, # family (Family)\n",
    "\t42, # friend (Friends)\n",
    "\t43, # female (Female)\n",
    "\t44, # male (Male)\n",
    "50, # cogproc (Cognitive Processes)\n",
    "\t51, # insight (Insight)\n",
    "\t52, # cause (Causal)\n",
    "\t53, # discrep (Discrepancies)\n",
    "\t54, # tentat (Tentative)\n",
    "\t55, # certain (Certainty)\n",
    "\t56, # differ (Differentiation)\n",
    "60, # percept (Perceptual Processes)\n",
    "\t61, # see (See)\n",
    "\t62, # hear (Hear)\n",
    "\t63, # feel (Feel)\n",
    "70, # bio (Biological Processes)\n",
    "\t71, # body (Body)\n",
    "\t72, # health (Health)\n",
    "\t73, # sexual (Sexual)\n",
    "\t74, # ingest (Ingest)\n",
    "80, # drives (Drives)\n",
    "\t81, # affiliation (Affiliation)\n",
    "\t82, # achieve (Achievement)\n",
    "\t83, # power (Power)\n",
    "\t84, # reward (Reward)\n",
    "\t85, # risk (Risk)\n",
    "100, # relativ (Relativity)\n",
    "\t101, # motion (Motion)\n",
    "\t102, # space (Space)\n",
    "\t103, # time (Time)\n",
    "# persconc (Personal Concerns)\n",
    "\t110, # work (Work)\n",
    "\t111, # leisure (Leisure)\n",
    "\t112, # home (Home)\n",
    "\t113, # money (Money)\n",
    "\t114, # relig (Religion)\n",
    "\t115, # death (Death)\n",
    "120, # informal (Informal Language)\n",
    "\t121, # swear (Swear)\n",
    "\t122, # netspeak (Netspeak)\n",
    "\t123, # assent (Assent)\n",
    "\t124, # nonflu (Nonfluencies)\n",
    "\t125, # filler (Filler Words)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting new_vocab_25_0.7\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_25_0.75\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_25_0.8\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_25_0.85\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_50_0.7\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_50_0.75\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_50_0.8\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_50_0.85\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_75_0.7\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_75_0.75\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_75_0.8\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_75_0.85\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_100_0.7\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_100_0.75\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_100_0.8\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting new_vocab_100_0.85\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n",
      "counting old_vocab\n",
      "  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "word_type_counts = {}\n",
    "for vocab_name in vocab_names:\n",
    "    print('counting', vocab_name)\n",
    "    vocab = vocabs[vocab_name]\n",
    "    word_counts[vocab_name] = {}\n",
    "    word_type_counts[vocab_name] = {}\n",
    "    for i in range(1, 16):\n",
    "        text_name = 'text_{:02}'.format(i)\n",
    "        print('  {:02}'.format(i), end='')\n",
    "        word_counts[vocab_name][text_name] = {}\n",
    "        word_type_counts[vocab_name][text_name] = {}\n",
    "        for word_type in word_types:\n",
    "            word_type_counts[vocab_name][text_name][word_type] = 0\n",
    "        text = texts[i-1]\n",
    "        for word in vocab:\n",
    "            word_to_check = word\n",
    "            word_to_check = word_to_check.replace('*', '.')\n",
    "            word_to_check = word_to_check.replace('(', '\\(')\n",
    "            word_to_check = word_to_check.replace(')', '\\)')\n",
    "            matches = re.findall(word_to_check, text)\n",
    "            if len(matches) > 0:\n",
    "                word_counts[vocab_name][text_name][word] = len(matches)\n",
    "                for word_type in vocab[word]:\n",
    "                    word_type_counts[vocab_name][text_name][int(word_type)] += len(matches)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./word_counts/word_counts.json', 'w') as f:\n",
    "    f.write(json.dumps(word_counts, ensure_ascii=False, indent=2))\n",
    "with open('./word_counts/word_type_counts.json', 'w') as f:\n",
    "    f.write(json.dumps(word_type_counts, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vocab_name in vocab_names:\n",
    "    df = pd.DataFrame(word_type_counts[vocab_name]).T\n",
    "    df.to_csv('./word_counts/{}.csv'.format(vocab_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
